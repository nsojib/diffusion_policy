{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on dp on both agentview and handview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import dill\n",
    "import numpy as np\n",
    "import collections\n",
    "import tqdm\n",
    "import imageio\n",
    "\n",
    "# Import workspace and utilities\n",
    "from diffusion_policy.workspace.train_diffusion_unet_hybrid_workspace import TrainDiffusionUnetHybridWorkspace\n",
    "from diffusion_policy.env_runner.robomimic_image_runner import RobomimicImageRunner\n",
    "from diffusion_policy.gym_util.async_vector_env import AsyncVectorEnv\n",
    "from diffusion_policy.gym_util.sync_vector_env import SyncVectorEnv\n",
    "from diffusion_policy.gym_util.multistep_wrapper import MultiStepWrapper\n",
    "from diffusion_policy.gym_util.video_recording_wrapper import VideoRecordingWrapper, VideoRecorder\n",
    "from diffusion_policy.model.common.rotation_transformer import RotationTransformer\n",
    "from diffusion_policy.policy.base_image_policy import BaseImagePolicy\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "from diffusion_policy.env_runner.base_image_runner import BaseImageRunner\n",
    "from diffusion_policy.env.robomimic.robomimic_image_wrapper import RobomimicImageWrapper\n",
    "\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "import robomimic.utils.env_utils as EnvUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "\n",
    "# ----- Helper Classes and Functions -----\n",
    "\n",
    "# Frame stacker for temporal context.\n",
    "class FrameStackForTrans:\n",
    "    def __init__(self, num_frames):\n",
    "        self.num_frames = num_frames\n",
    "        self.obs_history = {}\n",
    "    \n",
    "    def reset(self, init_obs):\n",
    "        self.obs_history = {}\n",
    "        for k in init_obs:\n",
    "            self.obs_history[k] = collections.deque([init_obs[k][None] for _ in range(self.num_frames)], maxlen=self.num_frames)\n",
    "        obs = {k: np.concatenate(self.obs_history[k], axis=0) for k in self.obs_history}\n",
    "        return obs\n",
    "    \n",
    "    def add_new_obs(self, new_obs):\n",
    "        for k in new_obs:\n",
    "            if 'timesteps' in k or 'actions' in k:\n",
    "                continue\n",
    "            self.obs_history[k].append(new_obs[k][None])\n",
    "        obs = {k: np.concatenate(self.obs_history[k], axis=0) for k in self.obs_history}\n",
    "        return obs\n",
    "\n",
    "# Environment wrapper to inject a dummy 'robot0_eye_in_hand_image'\n",
    "class DummyObsWrapper:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.required_key = 'robot0_eye_in_hand_image'\n",
    "        self.image_shape = (3, 84, 84)  # Adjust if needed\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        if self.required_key not in obs:\n",
    "            obs[self.required_key] = np.zeros(self.image_shape, dtype=np.float32)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        if self.required_key not in obs:\n",
    "            obs[self.required_key] = np.zeros(self.image_shape, dtype=np.float32)\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "# Create environment from metadata and shape configuration.\n",
    "def create_env(env_meta, shape_meta, enable_render=True):\n",
    "    modality_mapping = collections.defaultdict(list)\n",
    "    for key, attr in shape_meta['obs'].items():\n",
    "        modality_mapping[attr.get('type', 'low_dim')].append(key)\n",
    "    ObsUtils.initialize_obs_modality_mapping_from_dict(modality_mapping)\n",
    "    \n",
    "    env = EnvUtils.create_env_from_metadata(\n",
    "        env_meta=env_meta,\n",
    "        render=False,\n",
    "        render_offscreen=enable_render,\n",
    "        use_image_obs=enable_render,\n",
    "    )\n",
    "    return env\n",
    "\n",
    "# Undo the transformation on actions.\n",
    "def undo_transform_action(action, rotation_transformer):\n",
    "    raw_shape = action.shape\n",
    "    if raw_shape[-1] == 20:\n",
    "        action = action.reshape(-1, 2, 10)\n",
    "    d_rot = action.shape[-1] - 4\n",
    "    pos = action[..., :3]\n",
    "    rot = action[..., 3:3+d_rot]\n",
    "    gripper = action[..., -1:]\n",
    "    rot = rotation_transformer.inverse(rot)\n",
    "    uaction = np.concatenate([pos, rot, gripper], axis=-1)\n",
    "    if raw_shape[-1] == 20:\n",
    "        uaction = uaction.reshape(*raw_shape[:-1], 14)\n",
    "    return uaction\n",
    "\n",
    "# Rollout inference function.\n",
    "def rollout_diffusion(env, policy, rotation_transformer, n_obs_steps, n_action_steps, max_steps, return_imgs=False):\n",
    "    keys_select = ['robot0_eye_in_hand_image', 'agentview_image', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos']\n",
    "    imgs = []\n",
    "    imgs_eye = []\n",
    "    # np.random.seed(40)\n",
    "    # torch.manual_seed(40)\n",
    "    framestacker = FrameStackForTrans(n_obs_steps)\n",
    "    obs = env.reset()\n",
    "    # print(\"Rollout initial observation keys:\", list(obs.keys()))\n",
    "    policy.reset()\n",
    "    obs = framestacker.reset(obs)\n",
    "    done = False\n",
    "    success = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "        np_obs_dict = {key: obs[key][None, :] for key in keys_select if key in obs}\n",
    "        obs_dict = dict_apply(np_obs_dict, lambda x: torch.from_numpy(x).to(device))\n",
    "        with torch.no_grad():\n",
    "            action_dict = policy.predict_action(obs_dict)\n",
    "        np_action_dict = dict_apply(action_dict, lambda x: x.detach().cpu().numpy())\n",
    "        env_action = np_action_dict['action']\n",
    "        env_action = undo_transform_action(env_action, rotation_transformer)\n",
    "        env_action = env_action.squeeze()\n",
    "        # print(env_action)\n",
    "        for act in env_action:\n",
    "            act = act\n",
    "            if return_imgs:\n",
    "                img = env.render(mode=\"rgb_array\", height=512, width=512, camera_name=\"agentview\")\n",
    "                img_eye = env.render(mode=\"rgb_array\", height=512, width=512, camera_name=\"robot0_eye_in_hand\")\n",
    "                imgs.append(img)\n",
    "                imgs_eye.append(img_eye)\n",
    "            next_obs, reward, done, info = env.step(act)\n",
    "            success = env.is_success()[\"task\"]\n",
    "            step += 1\n",
    "            if step == max_steps:\n",
    "                done = True\n",
    "                break\n",
    "            obs = framestacker.add_new_obs(next_obs)\n",
    "            if done or success:\n",
    "                done = True\n",
    "                break\n",
    "        if done:\n",
    "            break\n",
    "    return success, imgs, imgs_eye\n",
    "\n",
    "# ----- Main Execution -----\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Set checkpoint and dataset paths (update these paths)\n",
    "checkpoint_path = \"/Riad/diffusion_policy/data/outputs/Riad_sim_lift_ph_full_2025_03_16_16_54_12/checkpoints/after_train_200_epochs.ckpt\"\n",
    "dataset_path = \"/Riad/diffusion_policy/full_image_low_lift_ph.hdf5\"  # Used only for env metadata.\n",
    "\n",
    "# Load checkpoint payload\n",
    "with open(checkpoint_path, 'rb') as f:\n",
    "    payload = torch.load(f, pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "\n",
    "# Build workspace and load the payload (model weights, etc.)\n",
    "workspace = TrainDiffusionUnetHybridWorkspace(cfg, output_dir=None)\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
    "\n",
    "# Select policy from workspace (use EMA model if enabled)\n",
    "policy = workspace.model\n",
    "if cfg.training.use_ema:\n",
    "    policy = workspace.ema_model\n",
    "policy.to(device)\n",
    "policy.eval()\n",
    "print(\"Policy loaded and set to eval mode.\")\n",
    "\n",
    "# Get environment metadata from dataset.\n",
    "env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path)\n",
    "env_meta['env_kwargs']['use_object_obs'] = False  # disable object state observation\n",
    "\n",
    "# Set absolute action mode if needed and initialize the rotation transformer.\n",
    "abs_action = True\n",
    "rotation_transformer = None\n",
    "if abs_action:\n",
    "    env_meta['env_kwargs']['controller_configs']['control_delta'] = True\n",
    "    rotation_transformer = RotationTransformer('axis_angle', 'rotation_6d')\n",
    "\n",
    "# Define shape metadata (include expected modalities)\n",
    "shape_meta = {\n",
    "    'obs': {\n",
    "        'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'},\n",
    "        'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'},\n",
    "        'robot0_eef_pos': {'shape': [3]},\n",
    "        'robot0_eef_quat': {'shape': [4]},\n",
    "        'robot0_gripper_qpos': {'shape': [2]},\n",
    "        'object': {'shape': [1]}  # adjust if needed\n",
    "    },\n",
    "    'action': {\n",
    "        'shape': [10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the environment.\n",
    "raw_env = create_env(env_meta=env_meta, shape_meta=shape_meta, enable_render=True)\n",
    "print(\"Created environment with name:\", env_meta.get(\"name\", \"Unknown\"))\n",
    "print(\"Action size is\", raw_env.action_space.shape[0] if hasattr(raw_env, \"action_space\") else \"Unknown\")\n",
    "print(\"Original env observation keys:\", list(raw_env.reset().keys()))\n",
    "\n",
    "# Wrap the environment to inject dummy 'robot0_eye_in_hand_image' if missing.\n",
    "env = DummyObsWrapper(raw_env)\n",
    "# print(\"Wrapped env observation keys:\", list(env.reset().keys()))\n",
    "\n",
    "# Inference parameters\n",
    "n_obs_steps = cfg.dataset_obs_steps if hasattr(cfg, \"dataset_obs_steps\") else 2\n",
    "n_action_steps = cfg.n_action_steps if hasattr(cfg, \"n_action_steps\") else 8\n",
    "max_steps = 400  # maximum steps per rollout\n",
    "n_trials = 5    # number of inference trials\n",
    "fps = 20         # frames per second for the output video\n",
    "\n",
    "# Run trials and save video for each trial.\n",
    "trial_success = []\n",
    "for i in range(n_trials):\n",
    "    print(f\"Trial {i+1}/{n_trials}...\")\n",
    "    # Set return_imgs=True to record frames.\n",
    "    success, imgs, imgs_eye= rollout_diffusion(env, policy, rotation_transformer, n_obs_steps, n_action_steps, max_steps, return_imgs=True)\n",
    "    trial_success.append(success)\n",
    "    print(f\"Trial {i+1} success: {success}\")\n",
    "    \n",
    "    # Save video if images were recorded.\n",
    "    if imgs:\n",
    "        video_filename = f\"trial_{i+1}_output.mp4\"\n",
    "        imageio.mimwrite(video_filename, imgs, fps=fps, quality=8)\n",
    "        print(f\"Saved video: {video_filename}\")\n",
    "    if imgs_eye:\n",
    "        video_filename_eye = f\"trial_{i+1}_output_eye.mp4\"\n",
    "        imageio.mimwrite(video_filename_eye, imgs_eye, fps=fps, quality=8)\n",
    "        print(f\"Saved video: {video_filename_eye}\")\n",
    "\n",
    "mean_success = np.mean(trial_success)\n",
    "print(\"Mean success over trials:\", mean_success)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy performance with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import dill\n",
    "import numpy as np\n",
    "import collections\n",
    "import tqdm\n",
    "import imageio\n",
    "\n",
    "# Import workspace and utilities\n",
    "from diffusion_policy.workspace.train_diffusion_unet_hybrid_workspace import TrainDiffusionUnetHybridWorkspace\n",
    "from diffusion_policy.env_runner.robomimic_image_runner import RobomimicImageRunner\n",
    "from diffusion_policy.gym_util.async_vector_env import AsyncVectorEnv\n",
    "from diffusion_policy.gym_util.sync_vector_env import SyncVectorEnv\n",
    "from diffusion_policy.gym_util.multistep_wrapper import MultiStepWrapper\n",
    "from diffusion_policy.gym_util.video_recording_wrapper import VideoRecordingWrapper, VideoRecorder\n",
    "from diffusion_policy.model.common.rotation_transformer import RotationTransformer\n",
    "from diffusion_policy.policy.base_image_policy import BaseImagePolicy\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "from diffusion_policy.env_runner.base_image_runner import BaseImageRunner\n",
    "from diffusion_policy.env.robomimic.robomimic_image_wrapper import RobomimicImageWrapper\n",
    "\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "import robomimic.utils.env_utils as EnvUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "\n",
    "# ----- Helper Classes and Functions -----\n",
    "\n",
    "# Frame stacker for temporal context.\n",
    "class FrameStackForTrans:\n",
    "    def __init__(self, num_frames):\n",
    "        self.num_frames = num_frames\n",
    "        self.obs_history = {}\n",
    "    \n",
    "    def reset(self, init_obs):\n",
    "        self.obs_history = {}\n",
    "        for k in init_obs:\n",
    "            self.obs_history[k] = collections.deque([init_obs[k][None] for _ in range(self.num_frames)], maxlen=self.num_frames)\n",
    "        obs = {k: np.concatenate(self.obs_history[k], axis=0) for k in self.obs_history}\n",
    "        return obs\n",
    "    \n",
    "    def add_new_obs(self, new_obs):\n",
    "        for k in new_obs:\n",
    "            if 'timesteps' in k or 'actions' in k:\n",
    "                continue\n",
    "            self.obs_history[k].append(new_obs[k][None])\n",
    "        obs = {k: np.concatenate(self.obs_history[k], axis=0) for k in self.obs_history}\n",
    "        return obs\n",
    "\n",
    "# Environment wrapper to inject a dummy 'robot0_eye_in_hand_image'\n",
    "class DummyObsWrapper:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.required_key = 'robot0_eye_in_hand_image'\n",
    "        self.image_shape = (3, 84, 84)  # Adjust if needed\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        if self.required_key not in obs:\n",
    "            obs[self.required_key] = np.zeros(self.image_shape, dtype=np.float32)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        if self.required_key not in obs:\n",
    "            obs[self.required_key] = np.zeros(self.image_shape, dtype=np.float32)\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "# Create environment from metadata and shape configuration.\n",
    "def create_env(env_meta, shape_meta, enable_render=True):\n",
    "    modality_mapping = collections.defaultdict(list)\n",
    "    for key, attr in shape_meta['obs'].items():\n",
    "        modality_mapping[attr.get('type', 'low_dim')].append(key)\n",
    "    ObsUtils.initialize_obs_modality_mapping_from_dict(modality_mapping)\n",
    "    \n",
    "    env = EnvUtils.create_env_from_metadata(\n",
    "        env_meta=env_meta,\n",
    "        render=False,\n",
    "        render_offscreen=enable_render,\n",
    "        use_image_obs=enable_render,\n",
    "    )\n",
    "    return env\n",
    "\n",
    "# Undo the transformation on actions.\n",
    "def undo_transform_action(action, rotation_transformer):\n",
    "    raw_shape = action.shape\n",
    "    if raw_shape[-1] == 20:\n",
    "        action = action.reshape(-1, 2, 10)\n",
    "    d_rot = action.shape[-1] - 4\n",
    "    pos = action[..., :3]\n",
    "    rot = action[..., 3:3+d_rot]\n",
    "    gripper = action[..., -1:]\n",
    "    rot = rotation_transformer.inverse(rot)\n",
    "    uaction = np.concatenate([pos, rot, gripper], axis=-1)\n",
    "    if raw_shape[-1] == 20:\n",
    "        uaction = uaction.reshape(*raw_shape[:-1], 14)\n",
    "    return uaction\n",
    "\n",
    "# Rollout inference function.\n",
    "def rollout_diffusion(env, policy, rotation_transformer, n_obs_steps, n_action_steps, max_steps, return_imgs=False):\n",
    "    keys_select = ['robot0_eye_in_hand_image', 'agentview_image', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos']\n",
    "    imgs = []\n",
    "    imgs_eye = []\n",
    "    # np.random.seed(40)\n",
    "    # torch.manual_seed(40)\n",
    "    framestacker = FrameStackForTrans(n_obs_steps)\n",
    "    obs = env.reset()\n",
    "    # print(\"Rollout initial observation keys:\", list(obs.keys()))\n",
    "    policy.reset()\n",
    "    obs = framestacker.reset(obs)\n",
    "    done = False\n",
    "    success = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "        np_obs_dict = {key: obs[key][None, :] for key in keys_select if key in obs}\n",
    "        obs_dict = dict_apply(np_obs_dict, lambda x: torch.from_numpy(x).to(device))\n",
    "        with torch.no_grad():\n",
    "            action_dict = policy.predict_action(obs_dict)\n",
    "        np_action_dict = dict_apply(action_dict, lambda x: x.detach().cpu().numpy())\n",
    "        env_action = np_action_dict['action']\n",
    "        env_action = undo_transform_action(env_action, rotation_transformer)\n",
    "        env_action = env_action.squeeze()\n",
    "        # print(env_action)\n",
    "        for act in env_action:\n",
    "            act = act\n",
    "            if return_imgs:\n",
    "                img = env.render(mode=\"rgb_array\", height=512, width=512, camera_name=\"agentview\")\n",
    "                img_eye = env.render(mode=\"rgb_array\", height=512, width=512, camera_name=\"robot0_eye_in_hand\")\n",
    "                imgs.append(img)\n",
    "                imgs_eye.append(img_eye)\n",
    "            noise = np.random.normal(loc=0.0, scale=0.4, size=act.shape)\n",
    "            act_noisy = act + noise\n",
    "            next_obs, reward, done, info = env.step(act_noisy)\n",
    "            success = env.is_success()[\"task\"]\n",
    "            step += 1\n",
    "            if step == max_steps:\n",
    "                done = True\n",
    "                break\n",
    "            obs = framestacker.add_new_obs(next_obs)\n",
    "            if done or success:\n",
    "                done = True\n",
    "                break\n",
    "        if done:\n",
    "            break\n",
    "    return success, imgs, imgs_eye\n",
    "\n",
    "# ----- Main Execution -----\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Set checkpoint and dataset paths (update these paths)\n",
    "checkpoint_path = \"/Riad/diffusion_policy/data/outputs/Riad_sim_lift_ph_full_2025_03_16_16_54_12/checkpoints/after_train_200_epochs.ckpt\"\n",
    "dataset_path = \"/Riad/diffusion_policy/full_image_low_lift_ph.hdf5\"  # Used only for env metadata.\n",
    "\n",
    "# Load checkpoint payload\n",
    "with open(checkpoint_path, 'rb') as f:\n",
    "    payload = torch.load(f, pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "\n",
    "# Build workspace and load the payload (model weights, etc.)\n",
    "workspace = TrainDiffusionUnetHybridWorkspace(cfg, output_dir=None)\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
    "\n",
    "# Select policy from workspace (use EMA model if enabled)\n",
    "policy = workspace.model\n",
    "if cfg.training.use_ema:\n",
    "    policy = workspace.ema_model\n",
    "policy.to(device)\n",
    "policy.eval()\n",
    "print(\"Policy loaded and set to eval mode.\")\n",
    "\n",
    "# Get environment metadata from dataset.\n",
    "env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path)\n",
    "env_meta['env_kwargs']['use_object_obs'] = False  # disable object state observation\n",
    "\n",
    "# Set absolute action mode if needed and initialize the rotation transformer.\n",
    "abs_action = True\n",
    "rotation_transformer = None\n",
    "if abs_action:\n",
    "    env_meta['env_kwargs']['controller_configs']['control_delta'] = True\n",
    "    rotation_transformer = RotationTransformer('axis_angle', 'rotation_6d')\n",
    "\n",
    "# Define shape metadata (include expected modalities)\n",
    "shape_meta = {\n",
    "    'obs': {\n",
    "        'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'},\n",
    "        'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'},\n",
    "        'robot0_eef_pos': {'shape': [3]},\n",
    "        'robot0_eef_quat': {'shape': [4]},\n",
    "        'robot0_gripper_qpos': {'shape': [2]},\n",
    "        'object': {'shape': [1]}  # adjust if needed\n",
    "    },\n",
    "    'action': {\n",
    "        'shape': [10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the environment.\n",
    "raw_env = create_env(env_meta=env_meta, shape_meta=shape_meta, enable_render=True)\n",
    "print(\"Created environment with name:\", env_meta.get(\"name\", \"Unknown\"))\n",
    "print(\"Action size is\", raw_env.action_space.shape[0] if hasattr(raw_env, \"action_space\") else \"Unknown\")\n",
    "print(\"Original env observation keys:\", list(raw_env.reset().keys()))\n",
    "\n",
    "# Wrap the environment to inject dummy 'robot0_eye_in_hand_image' if missing.\n",
    "env = DummyObsWrapper(raw_env)\n",
    "# print(\"Wrapped env observation keys:\", list(env.reset().keys()))\n",
    "\n",
    "# Inference parameters\n",
    "n_obs_steps = cfg.dataset_obs_steps if hasattr(cfg, \"dataset_obs_steps\") else 2\n",
    "n_action_steps = cfg.n_action_steps if hasattr(cfg, \"n_action_steps\") else 8\n",
    "max_steps = 400  # maximum steps per rollout\n",
    "n_trials = 5    # number of inference trials\n",
    "fps = 20         # frames per second for the output video\n",
    "\n",
    "# Run trials and save video for each trial.\n",
    "trial_success = []\n",
    "for i in range(n_trials):\n",
    "    print(f\"Trial {i+1}/{n_trials}...\")\n",
    "    # Set return_imgs=True to record frames.\n",
    "    success, imgs, imgs_eye= rollout_diffusion(env, policy, rotation_transformer, n_obs_steps, n_action_steps, max_steps, return_imgs=True)\n",
    "    trial_success.append(success)\n",
    "    print(f\"Trial {i+1} success: {success}\")\n",
    "    \n",
    "    # Save video if images were recorded.\n",
    "    if imgs:\n",
    "        video_filename = f\"trial_{i+1}_output.mp4\"\n",
    "        imageio.mimwrite(video_filename, imgs, fps=fps, quality=8)\n",
    "        print(f\"Saved video: {video_filename}\")\n",
    "    if imgs_eye:\n",
    "        video_filename_eye = f\"trial_{i+1}_output_eye.mp4\"\n",
    "        imageio.mimwrite(video_filename_eye, imgs_eye, fps=fps, quality=8)\n",
    "        print(f\"Saved video: {video_filename_eye}\")\n",
    "\n",
    "mean_success = np.mean(trial_success)\n",
    "print(\"Mean success over trials:\", mean_success)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on only agaentview or hand view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['robot0_eef_quat', 'robot0_gripper_qpos', 'robot0_eef_pos']\n",
      "using obs modality: rgb with keys: ['robot0_eye_in_hand_image']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carl_lab/miniconda3/envs/robodiff/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/carl_lab/miniconda3/envs/robodiff/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusion params: 1.737294e+07\n",
      "Vision params: 1.119709e+07\n",
      "Policy loaded and set to eval mode.\n",
      "Found 3 GPUs for rendering. Using device 0.\n",
      "Created environment with name Lift\n",
      "Action size is 7\n",
      "Created environment with name: Unknown\n",
      "Action size is Unknown\n",
      "Original env observation keys: ['robot0_eye_in_hand_image', 'object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos']\n",
      "Trial 1/10...\n",
      "Trial 1 success: True\n",
      "Saved video: trial_1_mh_hand_output.mp4\n",
      "Saved video: trial_1_mh_hand_output_eye.mp4\n",
      "Trial 2/10...\n",
      "Trial 2 success: True\n",
      "Saved video: trial_2_mh_hand_output.mp4\n",
      "Saved video: trial_2_mh_hand_output_eye.mp4\n",
      "Trial 3/10...\n",
      "Trial 3 success: True\n",
      "Saved video: trial_3_mh_hand_output.mp4\n",
      "Saved video: trial_3_mh_hand_output_eye.mp4\n",
      "Trial 4/10...\n",
      "Trial 4 success: True\n",
      "Saved video: trial_4_mh_hand_output.mp4\n",
      "Saved video: trial_4_mh_hand_output_eye.mp4\n",
      "Trial 5/10...\n",
      "Trial 5 success: True\n",
      "Saved video: trial_5_mh_hand_output.mp4\n",
      "Saved video: trial_5_mh_hand_output_eye.mp4\n",
      "Trial 6/10...\n",
      "Trial 6 success: True\n",
      "Saved video: trial_6_mh_hand_output.mp4\n",
      "Saved video: trial_6_mh_hand_output_eye.mp4\n",
      "Trial 7/10...\n",
      "Trial 7 success: True\n",
      "Saved video: trial_7_mh_hand_output.mp4\n",
      "Saved video: trial_7_mh_hand_output_eye.mp4\n",
      "Trial 8/10...\n",
      "Trial 8 success: True\n",
      "Saved video: trial_8_mh_hand_output.mp4\n",
      "Saved video: trial_8_mh_hand_output_eye.mp4\n",
      "Trial 9/10...\n",
      "Trial 9 success: True\n",
      "Saved video: trial_9_mh_hand_output.mp4\n",
      "Saved video: trial_9_mh_hand_output_eye.mp4\n",
      "Trial 10/10...\n",
      "Trial 10 success: True\n",
      "Saved video: trial_10_mh_hand_output.mp4\n",
      "Saved video: trial_10_mh_hand_output_eye.mp4\n",
      "Mean success over trials: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import dill\n",
    "import numpy as np\n",
    "import collections\n",
    "import tqdm\n",
    "import imageio\n",
    "\n",
    "# Import workspace and utilities\n",
    "from diffusion_policy.workspace.train_diffusion_unet_hybrid_workspace import TrainDiffusionUnetHybridWorkspace\n",
    "from diffusion_policy.env_runner.robomimic_image_runner import RobomimicImageRunner\n",
    "from diffusion_policy.gym_util.async_vector_env import AsyncVectorEnv\n",
    "from diffusion_policy.gym_util.sync_vector_env import SyncVectorEnv\n",
    "from diffusion_policy.gym_util.multistep_wrapper import MultiStepWrapper\n",
    "from diffusion_policy.gym_util.video_recording_wrapper import VideoRecordingWrapper, VideoRecorder\n",
    "from diffusion_policy.model.common.rotation_transformer import RotationTransformer\n",
    "from diffusion_policy.policy.base_image_policy import BaseImagePolicy\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "from diffusion_policy.env_runner.base_image_runner import BaseImageRunner\n",
    "from diffusion_policy.env.robomimic.robomimic_image_wrapper import RobomimicImageWrapper\n",
    "\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "import robomimic.utils.env_utils as EnvUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "\n",
    "# ----- Helper Classes and Functions -----\n",
    "\n",
    "# Frame stacker for temporal context.\n",
    "class FrameStackForTrans:\n",
    "    def __init__(self, num_frames):\n",
    "        self.num_frames = num_frames\n",
    "        self.obs_history = {}\n",
    "    \n",
    "    def reset(self, init_obs):\n",
    "        self.obs_history = {}\n",
    "        for k in init_obs:\n",
    "            self.obs_history[k] = collections.deque([init_obs[k][None] for _ in range(self.num_frames)], maxlen=self.num_frames)\n",
    "        obs = {k: np.concatenate(self.obs_history[k], axis=0) for k in self.obs_history}\n",
    "        return obs\n",
    "    \n",
    "    def add_new_obs(self, new_obs):\n",
    "        for k in new_obs:\n",
    "            if 'timesteps' in k or 'actions' in k:\n",
    "                continue\n",
    "            self.obs_history[k].append(new_obs[k][None])\n",
    "        obs = {k: np.concatenate(self.obs_history[k], axis=0) for k in self.obs_history}\n",
    "        return obs\n",
    "\n",
    "# Environment wrapper to inject a dummy 'robot0_eye_in_hand_image'\n",
    "class DummyObsWrapper:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.required_key = 'robot0_eye_in_hand_image'\n",
    "        self.image_shape = (3, 84, 84)  # Adjust if needed\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        if self.required_key not in obs:\n",
    "            obs[self.required_key] = np.zeros(self.image_shape, dtype=np.float32)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        if self.required_key not in obs:\n",
    "            obs[self.required_key] = np.zeros(self.image_shape, dtype=np.float32)\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "# Create environment from metadata and shape configuration.\n",
    "def create_env(env_meta, shape_meta, enable_render=True):\n",
    "    modality_mapping = collections.defaultdict(list)\n",
    "    for key, attr in shape_meta['obs'].items():\n",
    "        modality_mapping[attr.get('type', 'low_dim')].append(key)\n",
    "    ObsUtils.initialize_obs_modality_mapping_from_dict(modality_mapping)\n",
    "    \n",
    "    env = EnvUtils.create_env_from_metadata(\n",
    "        env_meta=env_meta,\n",
    "        render=False,\n",
    "        render_offscreen=enable_render,\n",
    "        use_image_obs=enable_render,\n",
    "    )\n",
    "    return env\n",
    "\n",
    "# Undo the transformation on actions.\n",
    "def undo_transform_action(action, rotation_transformer):\n",
    "    raw_shape = action.shape\n",
    "    if raw_shape[-1] == 20:\n",
    "        action = action.reshape(-1, 2, 10)\n",
    "    d_rot = action.shape[-1] - 4\n",
    "    pos = action[..., :3]\n",
    "    rot = action[..., 3:3+d_rot]\n",
    "    gripper = action[..., -1:]\n",
    "    rot = rotation_transformer.inverse(rot)\n",
    "    uaction = np.concatenate([pos, rot, gripper], axis=-1)\n",
    "    if raw_shape[-1] == 20:\n",
    "        uaction = uaction.reshape(*raw_shape[:-1], 14)\n",
    "    return uaction\n",
    "\n",
    "# Rollout inference function.\n",
    "def rollout_diffusion(env, policy, rotation_transformer, n_obs_steps, n_action_steps, max_steps, return_imgs=False):\n",
    "    keys_select = [ 'robot0_eye_in_hand_image', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos']\n",
    "    imgs = []\n",
    "    imgs_eye = []\n",
    "    # np.random.seed(40)\n",
    "    # torch.manual_seed(40)\n",
    "    framestacker = FrameStackForTrans(n_obs_steps)\n",
    "    obs = env.reset()\n",
    "    # print(\"Rollout initial observation keys:\", list(obs.keys()))\n",
    "    policy.reset()\n",
    "    obs = framestacker.reset(obs)\n",
    "    done = False\n",
    "    success = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "        np_obs_dict = {key: obs[key][None, :] for key in keys_select if key in obs}\n",
    "        obs_dict = dict_apply(np_obs_dict, lambda x: torch.from_numpy(x).to(device))\n",
    "        with torch.no_grad():\n",
    "            action_dict = policy.predict_action(obs_dict)\n",
    "        np_action_dict = dict_apply(action_dict, lambda x: x.detach().cpu().numpy())\n",
    "        env_action = np_action_dict['action']\n",
    "        env_action = undo_transform_action(env_action, rotation_transformer)\n",
    "        env_action = env_action.squeeze()\n",
    "        # print(env_action)\n",
    "        for act in env_action:\n",
    "            act = act\n",
    "            if return_imgs:\n",
    "                img = env.render(mode=\"rgb_array\", height=512, width=512, camera_name=\"agentview\")\n",
    "                img_eye = env.render(mode=\"rgb_array\", height=512, width=512, camera_name=\"robot0_eye_in_hand\")\n",
    "                imgs.append(img)\n",
    "                imgs_eye.append(img_eye)\n",
    "            # noise = np.random.normal(loc=0.0, scale=0.4, size=act.shape)\n",
    "            # act_noisy = act + noise\n",
    "            next_obs, reward, done, info = env.step(act)\n",
    "            success = env.is_success()[\"task\"]\n",
    "            step += 1\n",
    "            if step == max_steps:\n",
    "                done = True\n",
    "                break\n",
    "            obs = framestacker.add_new_obs(next_obs)\n",
    "            if done or success:\n",
    "                done = True\n",
    "                break\n",
    "        if done:\n",
    "            break\n",
    "    return success, imgs, imgs_eye\n",
    "\n",
    "# ----- Main Execution -----\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Set checkpoint and dataset paths (update these paths)\n",
    "checkpoint_path = \"/home/carl_lab/diffusion_policy/data/outputs/Riad_sim_lift_mh_vision_emp_hand_2025_03_22_00_50_32/checkpoints/after_train_200_epochs.ckpt\"\n",
    "dataset_path = \"/home/carl_lab/Riad/data/simulation/full_image_low_lift_ph.hdf5\"  # Used only for env metadata.\n",
    "\n",
    "# Load checkpoint payload\n",
    "with open(checkpoint_path, 'rb') as f:\n",
    "    payload = torch.load(f, pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "\n",
    "# Build workspace and load the payload (model weights, etc.)\n",
    "workspace = TrainDiffusionUnetHybridWorkspace(cfg, output_dir=None)\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
    "\n",
    "# Select policy from workspace (use EMA model if enabled)\n",
    "policy = workspace.model\n",
    "if cfg.training.use_ema:\n",
    "    policy = workspace.ema_model\n",
    "policy.to(device)\n",
    "policy.eval()\n",
    "print(\"Policy loaded and set to eval mode.\")\n",
    "\n",
    "# Get environment metadata from dataset.\n",
    "env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path)\n",
    "env_meta['env_kwargs']['use_object_obs'] = False  # disable object state observation\n",
    "\n",
    "# Set absolute action mode if needed and initialize the rotation transformer.\n",
    "abs_action = True\n",
    "rotation_transformer = None\n",
    "if abs_action:\n",
    "    env_meta['env_kwargs']['controller_configs']['control_delta'] = True\n",
    "    rotation_transformer = RotationTransformer('axis_angle', 'rotation_6d')\n",
    "\n",
    "# Define shape metadata (include expected modalities)\n",
    "shape_meta = {\n",
    "    'obs': {\n",
    "        'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'},\n",
    "        # 'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'},\n",
    "        'robot0_eef_pos': {'shape': [3]},\n",
    "        'robot0_eef_quat': {'shape': [4]},\n",
    "        'robot0_gripper_qpos': {'shape': [2]},\n",
    "        'object': {'shape': [1]}  # adjust if needed\n",
    "    },\n",
    "    'action': {\n",
    "        'shape': [10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the environment.\n",
    "raw_env = create_env(env_meta=env_meta, shape_meta=shape_meta, enable_render=True)\n",
    "print(\"Created environment with name:\", env_meta.get(\"name\", \"Unknown\"))\n",
    "print(\"Action size is\", raw_env.action_space.shape[0] if hasattr(raw_env, \"action_space\") else \"Unknown\")\n",
    "print(\"Original env observation keys:\", list(raw_env.reset().keys()))\n",
    "\n",
    "# Wrap the environment to inject dummy 'robot0_eye_in_hand_image' if missing.\n",
    "env = DummyObsWrapper(raw_env)\n",
    "# print(\"Wrapped env observation keys:\", list(env.reset().keys()))\n",
    "\n",
    "# Inference parameters\n",
    "n_obs_steps = cfg.dataset_obs_steps if hasattr(cfg, \"dataset_obs_steps\") else 2\n",
    "n_action_steps = cfg.n_action_steps if hasattr(cfg, \"n_action_steps\") else 8\n",
    "max_steps = 400  # maximum steps per rollout\n",
    "n_trials = 10   # number of inference trials\n",
    "fps = 20         # frames per second for the output video\n",
    "\n",
    "# Run trials and save video for each trial.\n",
    "trial_success = []\n",
    "for i in range(n_trials):\n",
    "    print(f\"Trial {i+1}/{n_trials}...\")\n",
    "    # Set return_imgs=True to record frames.\n",
    "    success, imgs, imgs_eye= rollout_diffusion(env, policy, rotation_transformer, n_obs_steps, n_action_steps, max_steps, return_imgs=True)\n",
    "    trial_success.append(success)\n",
    "    print(f\"Trial {i+1} success: {success}\")\n",
    "    \n",
    "    # Save video if images were recorded.\n",
    "    if imgs:\n",
    "        video_filename = f\"trial_{i+1}_mh_hand_output.mp4\"\n",
    "        imageio.mimwrite(video_filename, imgs, fps=fps, quality=8)\n",
    "        print(f\"Saved video: {video_filename}\")\n",
    "    if imgs_eye:\n",
    "        video_filename_eye = f\"trial_{i+1}_mh_hand_output_eye.mp4\"\n",
    "        imageio.mimwrite(video_filename_eye, imgs_eye, fps=fps, quality=8)\n",
    "        print(f\"Saved video: {video_filename_eye}\")\n",
    "\n",
    "mean_success = np.mean(trial_success)\n",
    "print(\"Mean success over trials:\", mean_success)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on only agaentview or hand view (noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['robot0_eef_quat', 'robot0_gripper_qpos', 'robot0_eef_pos']\n",
      "using obs modality: rgb with keys: ['robot0_eye_in_hand_image']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n",
      "Diffusion params: 1.737294e+07\n",
      "Vision params: 1.119709e+07\n",
      "Policy loaded and set to eval mode.\n",
      "Created environment with name Lift\n",
      "Action size is 7\n",
      "Created environment with name: Unknown\n",
      "Action size is Unknown\n",
      "Original env observation keys: ['robot0_eye_in_hand_image', 'object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos']\n",
      "Trial 1/5...\n",
      "Trial 1 success: False\n",
      "Saved video: results/48_my_trial_1_mh_hand_output.mp4\n",
      "Saved video: results/48_my_trial_1_mh_hand_output_eye.mp4\n",
      "Trial 2/5...\n",
      "Trial 2 success: False\n",
      "Saved video: results/48_my_trial_2_mh_hand_output.mp4\n",
      "Saved video: results/48_my_trial_2_mh_hand_output_eye.mp4\n",
      "Trial 3/5...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">228</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">225 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(n_trials):                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">226 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Trial {</span>i+<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span><span style=\"color: #808000; text-decoration-color: #808000\">}/{</span>n_trials<span style=\"color: #808000; text-decoration-color: #808000\">}...\"</span>)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">227 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Set return_imgs=True to record frames.</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>228 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>success, imgs, imgs_eye= rollout_diffusion(env, policy, rotation_transformer, n_obs_   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">229 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>trial_success.append(success)                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">230 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Trial {</span>i+<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span><span style=\"color: #808000; text-decoration-color: #808000\">} success: {</span>success<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">231 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">rollout_diffusion</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">124</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>np_obs_dict = {key: obs[key][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, :] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> key <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> keys_select <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> key <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> obs}        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>obs_dict = dict_apply(np_obs_dict, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">lambda</span> x: torch.from_numpy(x).to(device))       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>124 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>action_dict = policy.predict_action(obs_dict)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>np_action_dict = dict_apply(action_dict, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">lambda</span> x: x.detach().cpu().numpy())       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>env_action = np_action_dict[<span style=\"color: #808000; text-decoration-color: #808000\">'action'</span>]                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">127 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>env_action = undo_transform_action(env_action, rotation_transformer)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/carl_lab/diffusion_policy/diffusion_policy/policy/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">diffusion_unet_hybrid_image_policy_empha</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">sizing_vision.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">276</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict_action</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">273 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>cond_mask[:,:To,Da:] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">274 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">275 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># run sampling</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>276 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>nsample = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.conditional_sample(                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">277 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>cond_data,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">278 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>cond_mask,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">279 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>local_cond=local_cond,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/carl_lab/diffusion_policy/diffusion_policy/policy/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">diffusion_unet_hybrid_image_policy_empha</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">sizing_vision.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">217</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">conditional_sample</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">214 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>trajectory[condition_mask] = condition_data[condition_mask]                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">215 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">216 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># 2. predict model output</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>217 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model_output = model(trajectory, t, local_cond=local_cond, global_cond=globa   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">218 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">219 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># 3. compute previous image: x_t -&gt; x_t-1</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">220 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>trajectory = scheduler.step(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/carl_lab/miniconda3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/carl_lab/diffusion_policy/diffusion_policy/model/diffusion/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">conditional_unet1d.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">216</span> in    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">213 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = sample                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">214 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>h = []                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">215 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> idx, (resnet, resnet2, downsample) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.down_modules):            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>216 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = resnet(x, global_feature)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">217 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> idx == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(h_local) &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">218 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>x = x + h_local[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">219 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = resnet2(x, global_feature)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/carl_lab/miniconda3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/carl_lab/diffusion_policy/diffusion_policy/model/diffusion/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">conditional_unet1d.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">61</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 58 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>embed.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>], <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.out_channels, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 59 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>scale = embed[:,<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>,...]                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 60 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>bias = embed[:,<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>,...]                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 61 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>out = scale * out + bias                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 62 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 63 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>out = out + embed                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 64 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>out = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.blocks[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>](out)                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m228\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m225 \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(n_trials):                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mTrial \u001b[0m\u001b[33m{\u001b[0mi+\u001b[94m1\u001b[0m\u001b[33m}\u001b[0m\u001b[33m/\u001b[0m\u001b[33m{\u001b[0mn_trials\u001b[33m}\u001b[0m\u001b[33m...\u001b[0m\u001b[33m\"\u001b[0m)                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m227 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Set return_imgs=True to record frames.\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m228 \u001b[2m│   \u001b[0msuccess, imgs, imgs_eye= rollout_diffusion(env, policy, rotation_transformer, n_obs_   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m229 \u001b[0m\u001b[2m│   \u001b[0mtrial_success.append(success)                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m230 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mTrial \u001b[0m\u001b[33m{\u001b[0mi+\u001b[94m1\u001b[0m\u001b[33m}\u001b[0m\u001b[33m success: \u001b[0m\u001b[33m{\u001b[0msuccess\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m231 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mrollout_diffusion\u001b[0m:\u001b[94m124\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   \u001b[0mnp_obs_dict = {key: obs[key][\u001b[94mNone\u001b[0m, :] \u001b[94mfor\u001b[0m key \u001b[95min\u001b[0m keys_select \u001b[94mif\u001b[0m key \u001b[95min\u001b[0m obs}        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   │   \u001b[0mobs_dict = dict_apply(np_obs_dict, \u001b[94mlambda\u001b[0m x: torch.from_numpy(x).to(device))       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m123 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m124 \u001b[2m│   │   │   \u001b[0maction_dict = policy.predict_action(obs_dict)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   │   \u001b[0mnp_action_dict = dict_apply(action_dict, \u001b[94mlambda\u001b[0m x: x.detach().cpu().numpy())       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   \u001b[0menv_action = np_action_dict[\u001b[33m'\u001b[0m\u001b[33maction\u001b[0m\u001b[33m'\u001b[0m]                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m127 \u001b[0m\u001b[2m│   │   \u001b[0menv_action = undo_transform_action(env_action, rotation_transformer)               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/carl_lab/diffusion_policy/diffusion_policy/policy/\u001b[0m\u001b[1;33mdiffusion_unet_hybrid_image_policy_empha\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33msizing_vision.py\u001b[0m:\u001b[94m276\u001b[0m in \u001b[92mpredict_action\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m273 \u001b[0m\u001b[2m│   │   │   \u001b[0mcond_mask[:,:To,Da:] = \u001b[94mTrue\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m274 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m275 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# run sampling\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m276 \u001b[2m│   │   \u001b[0mnsample = \u001b[96mself\u001b[0m.conditional_sample(                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m277 \u001b[0m\u001b[2m│   │   │   \u001b[0mcond_data,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m278 \u001b[0m\u001b[2m│   │   │   \u001b[0mcond_mask,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m279 \u001b[0m\u001b[2m│   │   │   \u001b[0mlocal_cond=local_cond,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/carl_lab/diffusion_policy/diffusion_policy/policy/\u001b[0m\u001b[1;33mdiffusion_unet_hybrid_image_policy_empha\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33msizing_vision.py\u001b[0m:\u001b[94m217\u001b[0m in \u001b[92mconditional_sample\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m214 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrajectory[condition_mask] = condition_data[condition_mask]                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m215 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m216 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# 2. predict model output\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m217 \u001b[2m│   │   │   \u001b[0mmodel_output = model(trajectory, t, local_cond=local_cond, global_cond=globa   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m218 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m219 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# 3. compute previous image: x_t -> x_t-1\u001b[0m                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m220 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrajectory = scheduler.step(                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/carl_lab/miniconda3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m130\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1130 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/carl_lab/diffusion_policy/diffusion_policy/model/diffusion/\u001b[0m\u001b[1;33mconditional_unet1d.py\u001b[0m:\u001b[94m216\u001b[0m in    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m213 \u001b[0m\u001b[2m│   │   \u001b[0mx = sample                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m214 \u001b[0m\u001b[2m│   │   \u001b[0mh = []                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m215 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m idx, (resnet, resnet2, downsample) \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(\u001b[96mself\u001b[0m.down_modules):            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m216 \u001b[2m│   │   │   \u001b[0mx = resnet(x, global_feature)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m217 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m idx == \u001b[94m0\u001b[0m \u001b[95mand\u001b[0m \u001b[96mlen\u001b[0m(h_local) > \u001b[94m0\u001b[0m:                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m218 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mx = x + h_local[\u001b[94m0\u001b[0m]                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m219 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = resnet2(x, global_feature)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/carl_lab/miniconda3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m130\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1130 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/carl_lab/diffusion_policy/diffusion_policy/model/diffusion/\u001b[0m\u001b[1;33mconditional_unet1d.py\u001b[0m:\u001b[94m61\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 58 \u001b[0m\u001b[2m│   │   │   │   \u001b[0membed.shape[\u001b[94m0\u001b[0m], \u001b[94m2\u001b[0m, \u001b[96mself\u001b[0m.out_channels, \u001b[94m1\u001b[0m)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 59 \u001b[0m\u001b[2m│   │   │   \u001b[0mscale = embed[:,\u001b[94m0\u001b[0m,...]                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 60 \u001b[0m\u001b[2m│   │   │   \u001b[0mbias = embed[:,\u001b[94m1\u001b[0m,...]                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 61 \u001b[2m│   │   │   \u001b[0mout = scale * out + bias                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 62 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 63 \u001b[0m\u001b[2m│   │   │   \u001b[0mout = out + embed                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 64 \u001b[0m\u001b[2m│   │   \u001b[0mout = \u001b[96mself\u001b[0m.blocks[\u001b[94m1\u001b[0m](out)                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import dill\n",
    "import numpy as np\n",
    "import collections\n",
    "import tqdm\n",
    "import imageio\n",
    "\n",
    "# Import workspace and utilities\n",
    "from diffusion_policy.workspace.train_diffusion_unet_hybrid_workspace import TrainDiffusionUnetHybridWorkspace\n",
    "from diffusion_policy.env_runner.robomimic_image_runner import RobomimicImageRunner\n",
    "from diffusion_policy.gym_util.async_vector_env import AsyncVectorEnv\n",
    "from diffusion_policy.gym_util.sync_vector_env import SyncVectorEnv\n",
    "from diffusion_policy.gym_util.multistep_wrapper import MultiStepWrapper\n",
    "from diffusion_policy.gym_util.video_recording_wrapper import VideoRecordingWrapper, VideoRecorder\n",
    "from diffusion_policy.model.common.rotation_transformer import RotationTransformer\n",
    "from diffusion_policy.policy.base_image_policy import BaseImagePolicy\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "from diffusion_policy.env_runner.base_image_runner import BaseImageRunner\n",
    "from diffusion_policy.env.robomimic.robomimic_image_wrapper import RobomimicImageWrapper\n",
    "\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "import robomimic.utils.env_utils as EnvUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "\n",
    "# ----- Helper Classes and Functions -----\n",
    "\n",
    "# Frame stacker for temporal context.\n",
    "class FrameStackForTrans:\n",
    "    def __init__(self, num_frames):\n",
    "        self.num_frames = num_frames\n",
    "        self.obs_history = {}\n",
    "    \n",
    "    def reset(self, init_obs):\n",
    "        self.obs_history = {}\n",
    "        for k in init_obs:\n",
    "            self.obs_history[k] = collections.deque([init_obs[k][None] for _ in range(self.num_frames)], maxlen=self.num_frames)\n",
    "        obs = {k: np.concatenate(self.obs_history[k], axis=0) for k in self.obs_history}\n",
    "        return obs\n",
    "    \n",
    "    def add_new_obs(self, new_obs):\n",
    "        for k in new_obs:\n",
    "            if 'timesteps' in k or 'actions' in k:\n",
    "                continue\n",
    "            self.obs_history[k].append(new_obs[k][None])\n",
    "        obs = {k: np.concatenate(self.obs_history[k], axis=0) for k in self.obs_history}\n",
    "        return obs\n",
    "\n",
    "# Environment wrapper to inject a dummy 'robot0_eye_in_hand_image'\n",
    "class DummyObsWrapper:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.required_key = 'robot0_eye_in_hand_image'\n",
    "        self.image_shape = (3, 84, 84)  # Adjust if needed\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        if self.required_key not in obs:\n",
    "            obs[self.required_key] = np.zeros(self.image_shape, dtype=np.float32)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        if self.required_key not in obs:\n",
    "            obs[self.required_key] = np.zeros(self.image_shape, dtype=np.float32)\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "# Create environment from metadata and shape configuration.\n",
    "def create_env(env_meta, shape_meta, enable_render=True):\n",
    "    modality_mapping = collections.defaultdict(list)\n",
    "    for key, attr in shape_meta['obs'].items():\n",
    "        modality_mapping[attr.get('type', 'low_dim')].append(key)\n",
    "    ObsUtils.initialize_obs_modality_mapping_from_dict(modality_mapping)\n",
    "    \n",
    "    env = EnvUtils.create_env_from_metadata(\n",
    "        env_meta=env_meta,\n",
    "        render=False,\n",
    "        render_offscreen=enable_render,\n",
    "        use_image_obs=enable_render,\n",
    "    )\n",
    "    return env\n",
    "\n",
    "# Undo the transformation on actions.\n",
    "def undo_transform_action(action, rotation_transformer):\n",
    "    raw_shape = action.shape\n",
    "    if raw_shape[-1] == 20:\n",
    "        action = action.reshape(-1, 2, 10)\n",
    "    d_rot = action.shape[-1] - 4\n",
    "    pos = action[..., :3]\n",
    "    rot = action[..., 3:3+d_rot]\n",
    "    gripper = action[..., -1:]\n",
    "    rot = rotation_transformer.inverse(rot)\n",
    "    uaction = np.concatenate([pos, rot, gripper], axis=-1)\n",
    "    if raw_shape[-1] == 20:\n",
    "        uaction = uaction.reshape(*raw_shape[:-1], 14)\n",
    "    return uaction\n",
    "\n",
    "# Rollout inference function.\n",
    "def rollout_diffusion(env, policy, rotation_transformer, n_obs_steps, n_action_steps, max_steps, return_imgs=False):\n",
    "    keys_select = [ 'robot0_eye_in_hand_image', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos']\n",
    "    imgs = []\n",
    "    imgs_eye = []\n",
    "    # np.random.seed(40)\n",
    "    # torch.manual_seed(40)\n",
    "    framestacker = FrameStackForTrans(n_obs_steps)\n",
    "    obs = env.reset()\n",
    "    # print(\"Rollout initial observation keys:\", list(obs.keys()))\n",
    "    policy.reset()\n",
    "    obs = framestacker.reset(obs)\n",
    "    done = False\n",
    "    success = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "        np_obs_dict = {key: obs[key][None, :] for key in keys_select if key in obs}\n",
    "        obs_dict = dict_apply(np_obs_dict, lambda x: torch.from_numpy(x).to(device))\n",
    "        with torch.no_grad():\n",
    "            action_dict = policy.predict_action(obs_dict)\n",
    "        np_action_dict = dict_apply(action_dict, lambda x: x.detach().cpu().numpy())\n",
    "        env_action = np_action_dict['action']\n",
    "        env_action = undo_transform_action(env_action, rotation_transformer)\n",
    "        env_action = env_action.squeeze()\n",
    "        # print(env_action)\n",
    "        for act in env_action:\n",
    "            act = act\n",
    "            if return_imgs:\n",
    "                img = env.render(mode=\"rgb_array\", height=512, width=512, camera_name=\"agentview\")\n",
    "                img_eye = env.render(mode=\"rgb_array\", height=512, width=512, camera_name=\"robot0_eye_in_hand\")\n",
    "                imgs.append(img)\n",
    "                imgs_eye.append(img_eye)\n",
    "            noise = np.random.normal(loc=0.0, scale=0.4, size=act.shape)\n",
    "            act_noisy = act + noise\n",
    "            next_obs, reward, done, info = env.step(act_noisy)\n",
    "            success = env.is_success()[\"task\"]\n",
    "            step += 1\n",
    "            if step == max_steps:\n",
    "                done = True\n",
    "                break\n",
    "            obs = framestacker.add_new_obs(next_obs)\n",
    "            if done or success:\n",
    "                done = True\n",
    "                break\n",
    "        if done:\n",
    "            break\n",
    "    return success, imgs, imgs_eye\n",
    "\n",
    "# ----- Main Execution -----\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Set checkpoint and dataset paths (update these paths)\n",
    "checkpoint_path = \"/home/carl_lab/diffusion_policy/data/outputs/Riad_sim_lift_mh_vision_emp_hand_2025_03_22_00_50_32/checkpoints/after_train_200_epochs.ckpt\"\n",
    "dataset_path = \"/home/carl_lab/Riad/data/simulation/full_image_low_lift_ph.hdf5\"  # Used only for env metadata.\n",
    "\n",
    "# Load checkpoint payload\n",
    "with open(checkpoint_path, 'rb') as f:\n",
    "    payload = torch.load(f, pickle_module=dill)\n",
    "cfg = payload['cfg']\n",
    "\n",
    "# Build workspace and load the payload (model weights, etc.)\n",
    "workspace = TrainDiffusionUnetHybridWorkspace(cfg, output_dir=None)\n",
    "workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
    "\n",
    "# Select policy from workspace (use EMA model if enabled)\n",
    "policy = workspace.model\n",
    "if cfg.training.use_ema:\n",
    "    policy = workspace.ema_model\n",
    "policy.to(device)\n",
    "policy.eval()\n",
    "print(\"Policy loaded and set to eval mode.\")\n",
    "\n",
    "# Get environment metadata from dataset.\n",
    "env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path)\n",
    "env_meta['env_kwargs']['use_object_obs'] = False  # disable object state observation\n",
    "\n",
    "# Set absolute action mode if needed and initialize the rotation transformer.\n",
    "abs_action = True\n",
    "rotation_transformer = None\n",
    "if abs_action:\n",
    "    env_meta['env_kwargs']['controller_configs']['control_delta'] = True\n",
    "    rotation_transformer = RotationTransformer('axis_angle', 'rotation_6d')\n",
    "\n",
    "# Define shape metadata (include expected modalities)\n",
    "shape_meta = {\n",
    "    'obs': {\n",
    "        'robot0_eye_in_hand_image': {'shape': [3, 84, 84], 'type': 'rgb'},\n",
    "        # 'agentview_image': {'shape': [3, 84, 84], 'type': 'rgb'},\n",
    "        'robot0_eef_pos': {'shape': [3]},\n",
    "        'robot0_eef_quat': {'shape': [4]},\n",
    "        'robot0_gripper_qpos': {'shape': [2]},\n",
    "        'object': {'shape': [1]}  # adjust if needed\n",
    "    },\n",
    "    'action': {\n",
    "        'shape': [10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the environment.\n",
    "raw_env = create_env(env_meta=env_meta, shape_meta=shape_meta, enable_render=True)\n",
    "print(\"Created environment with name:\", env_meta.get(\"name\", \"Unknown\"))\n",
    "print(\"Action size is\", raw_env.action_space.shape[0] if hasattr(raw_env, \"action_space\") else \"Unknown\")\n",
    "print(\"Original env observation keys:\", list(raw_env.reset().keys()))\n",
    "\n",
    "# Wrap the environment to inject dummy 'robot0_eye_in_hand_image' if missing.\n",
    "env = DummyObsWrapper(raw_env)\n",
    "# print(\"Wrapped env observation keys:\", list(env.reset().keys()))\n",
    "\n",
    "# Inference parameters\n",
    "n_obs_steps = cfg.dataset_obs_steps if hasattr(cfg, \"dataset_obs_steps\") else 2\n",
    "n_action_steps = cfg.n_action_steps if hasattr(cfg, \"n_action_steps\") else 8\n",
    "max_steps = 700  # maximum steps per rollout\n",
    "n_trials = 5   # number of inference trials\n",
    "fps = 20         # frames per second for the output video\n",
    "\n",
    "# Run trials and save video for each trial.\n",
    "trial_success = []\n",
    "for i in range(n_trials):\n",
    "    print(f\"Trial {i+1}/{n_trials}...\")\n",
    "    # Set return_imgs=True to record frames.\n",
    "    success, imgs, imgs_eye= rollout_diffusion(env, policy, rotation_transformer, n_obs_steps, n_action_steps, max_steps, return_imgs=True)\n",
    "    trial_success.append(success)\n",
    "    print(f\"Trial {i+1} success: {success}\")\n",
    "    \n",
    "    # Save video if images were recorded.\n",
    "    if imgs:\n",
    "        video_filename = f\"results/48_my_trial_{i+1}_mh_hand_output.mp4\"\n",
    "        imageio.mimwrite(video_filename, imgs, fps=fps, quality=8)\n",
    "        print(f\"Saved video: {video_filename}\")\n",
    "    if imgs_eye:\n",
    "        video_filename_eye = f\"results/48_my_trial_{i+1}_mh_hand_output_eye.mp4\"\n",
    "        imageio.mimwrite(video_filename_eye, imgs_eye, fps=fps, quality=8)\n",
    "        print(f\"Saved video: {video_filename_eye}\")\n",
    "\n",
    "mean_success = np.mean(trial_success)\n",
    "print(\"Mean success over trials:\", mean_success)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
