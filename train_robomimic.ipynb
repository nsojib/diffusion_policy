{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# use line-buffering for both stdout and stderr\n",
    "# sys.stdout = open(sys.stdout.fileno(), mode='w', buffering=1)\n",
    "# sys.stderr = open(sys.stderr.fileno(), mode='w', buffering=1)\n",
    "\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "import pathlib\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    " \n",
    "import os\n",
    "import hydra\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import pathlib\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import random\n",
    "import wandb\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "# from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.policy.robomimic_lowdim_policy import RobomimicLowdimPolicy\n",
    "from diffusion_policy.dataset.base_dataset import BaseLowdimDataset\n",
    "from diffusion_policy.env_runner.base_lowdim_runner import BaseLowdimRunner\n",
    "from diffusion_policy.common.checkpoint_util import TopKCheckpointManager\n",
    "from diffusion_policy.common.json_logger import JsonLogger\n",
    "from diffusion_policy.common.pytorch_util import dict_apply, optimizer_to\n",
    "\n",
    "from diffusion_policy.policy.robomimic_lowdim_policy import RobomimicLowdimPolicy\n",
    "from diffusion_policy.policy.robomimic_image_policy import RobomimicImagePolicy\n",
    "from diffusion_policy.dataset.base_dataset import BaseImageDataset\n",
    "from diffusion_policy.env_runner.base_image_runner import BaseImageRunner\n",
    "\n",
    "# from diffusion_policy.workspace.train_diffusion_unet_hybrid_workspace import TrainDiffusionUnetHybridWorkspace\n",
    "import os\n",
    "import hydra\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import pathlib\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import random\n",
    "import wandb\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.policy.diffusion_unet_hybrid_image_policy import DiffusionUnetHybridImagePolicy\n",
    "from diffusion_policy.dataset.base_dataset import BaseImageDataset\n",
    "from diffusion_policy.env_runner.base_image_runner import BaseImageRunner\n",
    "from diffusion_policy.common.checkpoint_util import TopKCheckpointManager\n",
    "from diffusion_policy.common.json_logger import JsonLogger\n",
    "from diffusion_policy.common.pytorch_util import dict_apply, optimizer_to\n",
    "from diffusion_policy.model.diffusion.ema_model import EMAModel\n",
    "from diffusion_policy.model.common.lr_scheduler import get_scheduler\n",
    "\n",
    "OmegaConf.register_new_resolver(\"eval\", eval, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_policy.env_runner.robomimic_image_runner import RobomimicImageRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mujoco                    2.3.2 (mimicgen in another computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path='.'\n",
    "# config_name = 'image_square_mh_diffusion_policy_cnn_worse.yaml'\n",
    "config_name = \"image_square_mh_diffusion_policy_cnn_g40b30.yaml\"\n",
    "hdf5_filter_key = \"g40b30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=config_path):\n",
    "    cfg_org = compose(\n",
    "        config_name=config_name,\n",
    "        overrides=[\n",
    "            \"hydra.run.dir=data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}\",\n",
    "            \"training.seed=42\",\n",
    "            \"training.device=cuda:0\"\n",
    "        ],\n",
    "    )\n",
    "    print(cfg_org)\n",
    "    \n",
    "OmegaConf.resolve(cfg_org)\n",
    "\n",
    "print('resume: ', cfg_org.training.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_checkpoint_dir = \"/home/carl_lab/diffusion_policy/data/outputs/2024.12.13/03.05.17_train_diffusion_unet_hybrid_square_image/\"\n",
    "last_checkpoint_dir = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDiffusionUnetHybridWorkspace(BaseWorkspace):\n",
    "    include_keys = ['global_step', 'epoch']\n",
    "\n",
    "    def __init__(self, cfg: OmegaConf, output_dir=None):\n",
    "        super().__init__(cfg, output_dir=output_dir)\n",
    "\n",
    "        # set seed\n",
    "        seed = cfg.training.seed\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # configure model\n",
    "        self.model: DiffusionUnetHybridImagePolicy = hydra.utils.instantiate(cfg.policy)\n",
    "\n",
    "        self.ema_model: DiffusionUnetHybridImagePolicy = None\n",
    "        if cfg.training.use_ema:\n",
    "            self.ema_model = copy.deepcopy(self.model)\n",
    "\n",
    "        # configure training state\n",
    "        self.optimizer = hydra.utils.instantiate(\n",
    "            cfg.optimizer, params=self.model.parameters())\n",
    "\n",
    "        # configure training state\n",
    "        self.global_step = 0\n",
    "        self.epoch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workspace = TrainDiffusionUnetHybridWorkspace(cfg, output_dir=last_checkpoint_dir)\n",
    "# # workspace.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastest_ckpt_path = workspace.get_checkpoint_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/carl_lab/miniconda3/envs/robodiff/lib/python3.9/site-packages/mujoco_py/binaries/linux/mujoco210/bin/libglewegl.so\n",
    "# /home/carl_lab/.mujoco/mujoco210/bin/libglewegl.so\n",
    "# /home/carl_lab/.mujoco/mjpro150/bin/libglewegl.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['LD_LIBRARY_PATH'] = os.environ.get('LD_LIBRARY_PATH', '') + ':/home/carl_lab/.mujoco/mujoco210/bin'\n",
    "# os.environ['LD_LIBRARY_PATH'] = os.environ.get('LD_LIBRARY_PATH', '') + ':/usr/lib/nvidia'\n",
    "# os.environ['LD_LIBRARY_PATH'] = os.environ.get('LD_LIBRARY_PATH', '') + ':/usr/lib/x86_64-linux-gnu'\n",
    "# os.environ['LD_LIBRARY_PATH'] = os.environ.get('LD_LIBRARY_PATH', '') + ':/home/carl_lab/.mujoco/mujoco210/bin/libglewegl.so'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/carl_lab/.mujoco/mujoco210/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recreating workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = copy.deepcopy(cfg_org)\n",
    "\n",
    "seed = cfg.training.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# configure model\n",
    "model = hydra.utils.instantiate(cfg.policy)\n",
    "\n",
    "ema_model = None\n",
    "if cfg.training.use_ema:\n",
    "    ema_model = copy.deepcopy(model)\n",
    "\n",
    "# configure training state\n",
    "optimizer = hydra.utils.instantiate(\n",
    "    cfg.optimizer, params=model.parameters())\n",
    "\n",
    "# configure training state\n",
    "global_step = 0\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ws.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/carl_lab/diffusion_policy/data/outputs/custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# # resume training  \n",
    "# if cfg.training.resume:\n",
    "#     lastest_ckpt_path = get_checkpoint_path()\n",
    "#     if lastest_ckpt_path.is_file():\n",
    "#         print(f\"Resuming from checkpoint {lastest_ckpt_path}\")\n",
    "#         load_checkpoint(path=lastest_ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure dataset\n",
    "dataset: BaseImageDataset\n",
    "dataset = hydra.utils.instantiate(cfg.task.dataset)\n",
    "assert isinstance(dataset, BaseImageDataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, **cfg.dataloader)\n",
    "normalizer = dataset.get_normalizer()\n",
    "\n",
    "# configure validation dataset\n",
    "val_dataset = dataset.get_validation_dataset()\n",
    "val_dataloader = DataLoader(val_dataset, **cfg.val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_normalizer(normalizer)\n",
    "if cfg.training.use_ema:\n",
    "    ema_model.set_normalizer(normalizer)\n",
    "    \n",
    "# configure lr scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    cfg.training.lr_scheduler,\n",
    "    optimizer= optimizer,\n",
    "    num_warmup_steps=cfg.training.lr_warmup_steps,\n",
    "    num_training_steps=(\n",
    "        len(train_dataloader) * cfg.training.num_epochs) \\\n",
    "            // cfg.training.gradient_accumulate_every,\n",
    "    # pytorch assumes stepping LRScheduler every epoch\n",
    "    # however huggingface diffusers steps it every batch\n",
    "    last_epoch= global_step-1\n",
    ")\n",
    "\n",
    "# configure ema\n",
    "ema: EMAModel = None\n",
    "if cfg.training.use_ema:\n",
    "    ema = hydra.utils.instantiate(\n",
    "        cfg.ema,\n",
    "        model= ema_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.task.env_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_policy.env_runner.robomimic_image_runner import RobomimicImageRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure env\n",
    "env_runner: BaseImageRunner\n",
    "env_runner = hydra.utils.instantiate(\n",
    "    cfg.task.env_runner,\n",
    "    output_dir=output_dir)\n",
    "assert isinstance(env_runner, BaseImageRunner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"Cython<3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure checkpoint\n",
    "topk_manager = TopKCheckpointManager(\n",
    "    save_dir=os.path.join(output_dir, 'checkpoints'),\n",
    "    **cfg.checkpoint.topk\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device(cfg.training.device)\n",
    "model.to(device)\n",
    "if ema_model is not None:\n",
    "    ema_model.to(device)\n",
    "optimizer_to(optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.training.debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampling_batch = None\n",
    "log_path = os.path.join(output_dir, 'logs.json.txt')\n",
    "with JsonLogger(log_path) as json_logger:\n",
    "    for local_epoch_idx in range(cfg.training.num_epochs):\n",
    "        step_log = dict()\n",
    "        # ========= train for this epoch ==========\n",
    "        train_losses = list()\n",
    "        with tqdm.tqdm(train_dataloader, desc=f\"Training epoch {self.epoch}\", \n",
    "                leave=False, mininterval=cfg.training.tqdm_interval_sec) as tepoch:\n",
    "            for batch_idx, batch in enumerate(tepoch):\n",
    "                # device transfer\n",
    "                batch = dict_apply(batch, lambda x: x.to(device, non_blocking=True))\n",
    "                if train_sampling_batch is None:\n",
    "                    train_sampling_batch = batch\n",
    "\n",
    "                # compute loss\n",
    "                raw_loss =  model.compute_loss(batch)\n",
    "                loss = raw_loss / cfg.training.gradient_accumulate_every\n",
    "                loss.backward()\n",
    "\n",
    "                # step optimizer\n",
    "                if global_step % cfg.training.gradient_accumulate_every == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    lr_scheduler.step()\n",
    "                \n",
    "                # update ema\n",
    "                if cfg.training.use_ema:\n",
    "                    ema.step(model)\n",
    "\n",
    "                # logging\n",
    "                raw_loss_cpu = raw_loss.item()\n",
    "                tepoch.set_postfix(loss=raw_loss_cpu, refresh=False)\n",
    "                train_losses.append(raw_loss_cpu)\n",
    "                step_log = {\n",
    "                    'train_loss': raw_loss_cpu,\n",
    "                    'global_step': global_step,\n",
    "                    'epoch': epoch,\n",
    "                    'lr': lr_scheduler.get_last_lr()[0]\n",
    "                }\n",
    "\n",
    "                is_last_batch = (batch_idx == (len(train_dataloader)-1))\n",
    "                if not is_last_batch:\n",
    "                    # log of last step is combined with validation and rollout\n",
    "                    json_logger.log(step_log)\n",
    "                    global_step += 1\n",
    "\n",
    "                if (cfg.training.max_train_steps is not None) \\\n",
    "                    and batch_idx >= (cfg.training.max_train_steps-1):\n",
    "                    break\n",
    "\n",
    "        # at the end of each epoch\n",
    "        # replace train_loss with epoch average\n",
    "        train_loss = np.mean(train_losses)\n",
    "        step_log['train_loss'] = train_loss\n",
    "\n",
    "        # ========= eval for this epoch ==========\n",
    "        policy = model\n",
    "        if cfg.training.use_ema:\n",
    "            policy = ema_model\n",
    "        policy.eval()\n",
    "\n",
    "        # run rollout\n",
    "        if (epoch % cfg.training.rollout_every) == 0:\n",
    "            runner_log = env_runner.run(policy)\n",
    "            # log all\n",
    "            step_log.update(runner_log)\n",
    "\n",
    "        # run validation\n",
    "        if (epoch % cfg.training.val_every) == 0:\n",
    "            with torch.no_grad():\n",
    "                val_losses = list()\n",
    "                with tqdm.tqdm(val_dataloader, desc=f\"Validation epoch {epoch}\", \n",
    "                        leave=False, mininterval=cfg.training.tqdm_interval_sec) as tepoch:\n",
    "                    for batch_idx, batch in enumerate(tepoch):\n",
    "                        batch = dict_apply(batch, lambda x: x.to(device, non_blocking=True))\n",
    "                        loss = model.compute_loss(batch)\n",
    "                        val_losses.append(loss)\n",
    "                        if (cfg.training.max_val_steps is not None) \\\n",
    "                            and batch_idx >= (cfg.training.max_val_steps-1):\n",
    "                            break\n",
    "                if len(val_losses) > 0:\n",
    "                    val_loss = torch.mean(torch.tensor(val_losses)).item()\n",
    "                    # log epoch average validation loss\n",
    "                    step_log['val_loss'] = val_loss\n",
    "\n",
    "        # run diffusion sampling on a training batch\n",
    "        if (epoch % cfg.training.sample_every) == 0:\n",
    "            with torch.no_grad():\n",
    "                # sample trajectory from training set, and evaluate difference\n",
    "                batch = dict_apply(train_sampling_batch, lambda x: x.to(device, non_blocking=True))\n",
    "                obs_dict = batch['obs']\n",
    "                gt_action = batch['action']\n",
    "                \n",
    "                result = policy.predict_action(obs_dict)\n",
    "                pred_action = result['action_pred']\n",
    "                mse = torch.nn.functional.mse_loss(pred_action, gt_action)\n",
    "                step_log['train_action_mse_error'] = mse.item()\n",
    "                del batch\n",
    "                del obs_dict\n",
    "                del gt_action\n",
    "                del result\n",
    "                del pred_action\n",
    "                del mse\n",
    "        \n",
    "        # checkpoint\n",
    "        if (epoch % cfg.training.checkpoint_every) == 0:\n",
    "            # checkpointing\n",
    "            if cfg.checkpoint.save_last_ckpt:\n",
    "                save_checkpoint()\n",
    "            if cfg.checkpoint.save_last_snapshot:\n",
    "                save_snapshot()\n",
    "\n",
    "            # sanitize metric names\n",
    "            metric_dict = dict()\n",
    "            for key, value in step_log.items():\n",
    "                new_key = key.replace('/', '_')\n",
    "                metric_dict[new_key] = value\n",
    "            \n",
    "            # We can't copy the last checkpoint here\n",
    "            # since save_checkpoint uses threads.\n",
    "            # therefore at this point the file might have been empty!\n",
    "            topk_ckpt_path = topk_manager.get_ckpt_path(metric_dict)\n",
    "\n",
    "            if topk_ckpt_path is not None:\n",
    "                save_checkpoint(path=topk_ckpt_path)\n",
    "        # ========= eval end for this epoch ==========\n",
    "        policy.train()\n",
    "\n",
    "        # end of epoch\n",
    "        # log of last step is combined with validation and rollout\n",
    "        json_logger.log(step_log)\n",
    "        global_step += 1\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
