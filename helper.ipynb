{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca591e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ns1254/miniconda3/envs/robodiff/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No private macro file found!\n",
      "    It is recommended to use a private macro file\n",
      "    To setup, run: python /home/ns1254/diffusion_policy/envs/robomimic/robomimic/scripts/setup_macros.py\n",
      ")\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# use line-buffering for both stdout and stderr\n",
    "# sys.stdout = open(sys.stdout.fileno(), mode='w', buffering=1)\n",
    "# sys.stderr = open(sys.stderr.fileno(), mode='w', buffering=1)\n",
    "\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "import pathlib\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    " \n",
    "import os\n",
    "import hydra\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import pathlib\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import random\n",
    "import wandb\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "# from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.policy.robomimic_lowdim_policy import RobomimicLowdimPolicy\n",
    "from diffusion_policy.dataset.base_dataset import BaseLowdimDataset\n",
    "from diffusion_policy.env_runner.base_lowdim_runner import BaseLowdimRunner\n",
    "from diffusion_policy.common.checkpoint_util import TopKCheckpointManager\n",
    "from diffusion_policy.common.json_logger import JsonLogger\n",
    "from diffusion_policy.common.pytorch_util import dict_apply, optimizer_to\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dc3654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OmegaConf.register_new_resolver(\"eval\", eval, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6e12e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_target_': 'diffusion_policy.workspace.train_robomimic_lowdim_workspace.TrainRobomimicLowdimWorkspace', 'action_dim': 7, 'checkpoint': {'save_last_ckpt': True, 'save_last_snapshot': False, 'topk': {'format_str': 'epoch={epoch:04d}-test_mean_score={test_mean_score:.3f}.ckpt', 'k': 5, 'mode': 'max', 'monitor_key': 'test_mean_score'}}, 'dataloader': {'batch_size': 256, 'num_workers': 1, 'persistent_workers': False, 'pin_memory': True, 'shuffle': True}, 'exp_name': 'default', 'horizon': 1, 'keypoint_visible_rate': 1.0, 'logging': {'group': None, 'id': None, 'mode': 'online', 'name': '2022.12.30-01.51.31_train_robomimic_lowdim_can_lowdim', 'project': 'diffusion_policy_debug', 'resume': True, 'tags': ['train_robomimic_lowdim', 'can_lowdim', 'default']}, 'multi_run': {'run_dir': 'data/outputs/2022.12.30/01.51.31_train_robomimic_lowdim_can_lowdim', 'wandb_name_base': '2022.12.30-01.51.31_train_robomimic_lowdim_can_lowdim'}, 'n_action_steps': 1, 'n_latency_steps': 0, 'n_obs_steps': 1, 'name': 'train_robomimic_lowdim', 'obs_dim': 23, 'past_action_visible': False, 'policy': {'_target_': 'diffusion_policy.policy.robomimic_lowdim_policy.RobomimicLowdimPolicy', 'action_dim': 7, 'algo_name': 'bc', 'dataset_type': 'mh', 'obs_dim': 23, 'obs_type': 'low_dim', 'task_name': 'can'}, 'task': {'abs_action': False, 'action_dim': 7, 'dataset': {'_target_': 'diffusion_policy.dataset.robomimic_replay_lowdim_dataset.RobomimicReplayLowdimDataset', 'abs_action': False, 'dataset_path': '/home/ns1254/diffusion_policy/data/robomimic/datasets/can/mh/can_mh_image.hdf5', 'horizon': 1, 'obs_keys': ['object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos'], 'pad_after': 0, 'pad_before': 0, 'seed': 42, 'val_ratio': 0.02}, 'dataset_path': '/home/ns1254/diffusion_policy/data/robomimic/datasets/can/mh/can_mh_image.hdf5', 'dataset_type': 'mh', 'env_runner': {'_target_': 'diffusion_policy.env_runner.robomimic_lowdim_runner.RobomimicLowdimRunner', 'abs_action': False, 'crf': 22, 'dataset_path': '/home/ns1254/diffusion_policy/data/robomimic/datasets/can/mh/can_mh_image.hdf5', 'fps': 10, 'max_steps': 500, 'n_action_steps': 1, 'n_envs': 28, 'n_latency_steps': 0, 'n_obs_steps': 1, 'n_test': 50, 'n_test_vis': 4, 'n_train': 6, 'n_train_vis': 2, 'obs_keys': ['object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos'], 'past_action': False, 'render_hw': [128, 128], 'test_start_seed': 100000, 'train_start_idx': 0}, 'keypoint_dim': 3, 'name': 'can_lowdim', 'obs_dim': 23, 'obs_keys': ['object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos'], 'task_name': 'can'}, 'task_name': 'can_lowdim', 'training': {'checkpoint_every': 50, 'debug': False, 'device': 'cuda:0', 'max_train_steps': None, 'max_val_steps': None, 'num_epochs': 8000, 'resume': True, 'rollout_every': 50, 'sample_every': 5, 'seed': 42, 'tqdm_interval_sec': 1.0, 'val_every': 1}, 'transition_dim': 30, 'val_dataloader': {'batch_size': 256, 'num_workers': 1, 'persistent_workers': False, 'pin_memory': True, 'shuffle': False}}\n"
     ]
    }
   ],
   "source": [
    "# python train.py --config-dir=\"configs\" --config-name=config_lowdim_can_mh_bc.yaml training.seed=42 training.device=cuda:0 hydra.run.dir='data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}' +mask_fn=/home/carl_lab/ns/diffusion_policy/data/can_mh_bed_masked_0_60.txt +segments_toremove_file=\"/home/ns1254/gib/gib_results/gib_can_mh_image/subtask_rm120/segs_index_rm120_gib_can_mh_image.json\"\n",
    "\n",
    "config_file = \"config_lowdim_can_mh_bc.yaml\"\n",
    "\n",
    "config_path='configs'\n",
    "\n",
    "OmegaConf.register_new_resolver(\"eval\", eval, replace=True)\n",
    "\n",
    "with initialize(version_base=None, config_path=config_path):\n",
    "    cfg = compose(config_name=config_file, overrides=[])\n",
    "    print(cfg)\n",
    "\n",
    "OmegaConf.resolve(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce0af9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using segs file:  /home/ns1254/gib/gib_results/gib_can_mh_image/subtask_rm120/segs_index_rm120_gib_can_mh_image.json\n",
      "segs_toremove: {'demo_228': [[0, 119], [119, 385]], 'demo_243': [[0, 160], [160, 302]], 'demo_202': [[0, 299], [299, 397]], 'demo_122': [[0, 83]], 'demo_201': [[0, 125], [125, 531]], 'demo_71': [[0, 111], [111, 605]], 'demo_203': [[0, 131], [131, 261]], 'demo_208': [[0, 290], [290, 706]], 'demo_226': [[0, 196], [196, 286]], 'demo_116': [[0, 70]], 'demo_151': [[0, 80]], 'demo_248': [[248, 597]], 'demo_216': [[0, 118], [118, 206]], 'demo_209': [[0, 197], [197, 335]], 'demo_220': [[0, 150], [150, 270]], 'demo_234': [[0, 160], [160, 257]], 'demo_64': [[129, 1050]], 'demo_69': [[0, 286], [286, 914]], 'demo_87': [[0, 174], [174, 328]], 'demo_80': [[0, 206], [206, 335]], 'demo_219': [[0, 186]], 'demo_229': [[71, 488]], 'demo_273': [[140, 398]], 'demo_70': [[0, 177], [177, 303]], 'demo_236': [[0, 136], [136, 225]], 'demo_244': [[0, 351], [351, 445]], 'demo_65': [[117, 243]], 'demo_128': [[0, 66], [66, 130]], 'demo_163': [[0, 102]], 'demo_194': [[102, 168]], 'demo_86': [[121, 450]], 'demo_214': [[0, 245], [245, 329]], 'demo_50': [[159, 310]], 'demo_176': [[0, 88]], 'demo_284': [[0, 147], [147, 270]], 'demo_210': [[0, 119], [119, 287]], 'demo_217': [[0, 396], [396, 614]], 'demo_96': [[0, 139], [139, 289]], 'demo_206': [[0, 134], [134, 321]], 'demo_68': [[151, 253]], 'demo_165': [[85, 169]], 'demo_186': [[0, 70]], 'demo_12': [[0, 56], [56, 113]], 'demo_62': [[0, 143], [143, 524]], 'demo_200': [[0, 413]], 'demo_221': [[0, 167]], 'demo_245': [[62, 532]], 'demo_247': [[0, 125]], 'demo_73': [[0, 151]], 'demo_100': [[75, 156]], 'demo_112': [[0, 65]], 'demo_127': [[0, 61]], 'demo_140': [[0, 58]], 'demo_180': [[0, 70], [70, 132]], 'demo_191': [[94, 173]], 'demo_107': [[0, 124]], 'demo_204': [[0, 143], [143, 351]], 'demo_28': [[65, 109]], 'demo_61': [[0, 212], [212, 370]], 'demo_66': [[0, 145]], 'demo_90': [[0, 141]], 'demo_173': [[0, 95]], 'demo_205': [[116, 224]], 'demo_285': [[95, 176]], 'demo_20': [[0, 55], [55, 107]], 'demo_45': [[54, 107]], 'demo_131': [[78, 167]], 'demo_211': [[0, 238]], 'demo_231': [[0, 167], [167, 241]], 'demo_292': [[0, 137]], 'demo_22': [[63, 136]], 'demo_82': [[0, 169]], 'demo_92': [[0, 167]], 'demo_125': [[0, 70]], 'demo_199': [[0, 72]], 'demo_242': [[0, 149]], 'demo_29': [[67, 128]], 'demo_136': [[46, 110]], 'demo_137': [[0, 61], [61, 130]], 'demo_183': [[70, 207]], 'demo_2': [[0, 66]], 'demo_9': [[57, 103]], 'demo_46': [[62, 120]], 'demo_47': [[0, 53]], 'demo_49': [[85, 148]], 'demo_72': [[176, 337]], 'demo_99': [[154, 305]]}\n"
     ]
    }
   ],
   "source": [
    "segs_fn=\"/home/ns1254/gib/gib_results/gib_can_mh_image/subtask_rm120/segs_index_rm120_gib_can_mh_image.json\"\n",
    "segments_toremove_file = segs_fn\n",
    "if hasattr(cfg, 'segments_toremove_file'):\n",
    "    segments_toremove_file = cfg.segments_toremove_file\n",
    "    if segments_toremove_file is None:\n",
    "        segments_toremove_file=\"\"\n",
    "    \n",
    "if os.path.exists(segments_toremove_file):\n",
    "    print('using segs file: ', segments_toremove_file)\n",
    "    with open(segments_toremove_file, 'r') as f:\n",
    "        data = json.load(f) \n",
    "    segs_toremove = data['data'] \n",
    "else:\n",
    "    segs_toremove = {}  \n",
    "\n",
    "print(f\"segs_toremove: {segs_toremove}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfced5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diffusion_policy.workspace.train_robomimic_lowdim_workspace.TrainRobomimicLowdimWorkspace"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = hydra.utils.get_class(cfg._target_)\n",
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c252ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action_dim': 7,\n",
       " 'checkpoint': {'save_last_ckpt': True, 'save_last_snapshot': False, 'topk': {'format_str': 'epoch={epoch:04d}-test_mean_score={test_mean_score:.3f}.ckpt', 'k': 5, 'mode': 'max', 'monitor_key': 'test_mean_score'}},\n",
       " 'dataloader': {'batch_size': 256, 'num_workers': 1, 'persistent_workers': False, 'pin_memory': True, 'shuffle': True},\n",
       " 'exp_name': 'default',\n",
       " 'horizon': 1,\n",
       " 'keypoint_visible_rate': 1.0,\n",
       " 'logging': {'group': None, 'id': None, 'mode': 'online', 'name': '2022.12.30-01.51.31_train_robomimic_lowdim_can_lowdim', 'project': 'diffusion_policy_debug', 'resume': True, 'tags': ['train_robomimic_lowdim', 'can_lowdim', 'default']},\n",
       " 'multi_run': {'run_dir': 'data/outputs/2022.12.30/01.51.31_train_robomimic_lowdim_can_lowdim', 'wandb_name_base': '2022.12.30-01.51.31_train_robomimic_lowdim_can_lowdim'},\n",
       " 'n_action_steps': 1,\n",
       " 'n_latency_steps': 0,\n",
       " 'n_obs_steps': 1,\n",
       " 'name': 'train_robomimic_lowdim',\n",
       " 'obs_dim': 23,\n",
       " 'past_action_visible': False,\n",
       " 'policy': {'_target_': 'diffusion_policy.policy.robomimic_lowdim_policy.RobomimicLowdimPolicy', 'action_dim': 7, 'algo_name': 'bc', 'dataset_type': 'mh', 'obs_dim': 23, 'obs_type': 'low_dim', 'task_name': 'can'},\n",
       " 'task': {'abs_action': False, 'action_dim': 7, 'dataset': {'_target_': 'diffusion_policy.dataset.robomimic_replay_lowdim_dataset.RobomimicReplayLowdimDataset', 'abs_action': False, 'dataset_path': '/home/ns1254/diffusion_policy/data/robomimic/datasets/can/mh/can_mh_image.hdf5', 'horizon': 1, 'obs_keys': ['object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos'], 'pad_after': 0, 'pad_before': 0, 'seed': 42, 'val_ratio': 0.02}, 'dataset_path': '/home/ns1254/diffusion_policy/data/robomimic/datasets/can/mh/can_mh_image.hdf5', 'dataset_type': 'mh', 'env_runner': {'_target_': 'diffusion_policy.env_runner.robomimic_lowdim_runner.RobomimicLowdimRunner', 'abs_action': False, 'crf': 22, 'dataset_path': '/home/ns1254/diffusion_policy/data/robomimic/datasets/can/mh/can_mh_image.hdf5', 'fps': 10, 'max_steps': 500, 'n_action_steps': 1, 'n_envs': 28, 'n_latency_steps': 0, 'n_obs_steps': 1, 'n_test': 50, 'n_test_vis': 4, 'n_train': 6, 'n_train_vis': 2, 'obs_keys': ['object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos'], 'past_action': False, 'render_hw': [128, 128], 'test_start_seed': 100000, 'train_start_idx': 0}, 'keypoint_dim': 3, 'name': 'can_lowdim', 'obs_dim': 23, 'obs_keys': ['object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos'], 'task_name': 'can'},\n",
       " 'task_name': 'can_lowdim',\n",
       " 'training': {'checkpoint_every': 50, 'debug': False, 'device': 'cuda:0', 'max_train_steps': None, 'max_val_steps': None, 'num_epochs': 8000, 'resume': True, 'rollout_every': 50, 'sample_every': 5, 'seed': 42, 'tqdm_interval_sec': 1.0, 'val_every': 1},\n",
       " 'transition_dim': 30,\n",
       " 'val_dataloader': {'batch_size': 256, 'num_workers': 1, 'persistent_workers': False, 'pin_memory': True, 'shuffle': False}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cfg={key:value for key, value in cfg.items()}\n",
    "del new_cfg['_target_']\n",
    "new_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf27496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusion_policy.workspace.train_robomimic_lowdim_workspace import TrainRobomimicLowdimWorkspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f9923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import pathlib\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import random\n",
    "import wandb\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.policy.robomimic_lowdim_policy import RobomimicLowdimPolicy\n",
    "from diffusion_policy.dataset.base_dataset import BaseLowdimDataset\n",
    "from diffusion_policy.env_runner.base_lowdim_runner import BaseLowdimRunner\n",
    "from diffusion_policy.common.checkpoint_util import TopKCheckpointManager\n",
    "from diffusion_policy.common.json_logger import JsonLogger\n",
    "from diffusion_policy.common.pytorch_util import dict_apply, optimizer_to\n",
    "from torch.utils.data import DataLoader, Sampler \n",
    "\n",
    "OmegaConf.register_new_resolver(\"eval\", eval, replace=True)\n",
    "\n",
    "\n",
    "class CustomIndicesSampler(Sampler):\n",
    "    def __init__(self, custom_indices):\n",
    "        self.custom_indices = np.random.permutation(custom_indices)\n",
    "\n",
    "    def __iter__(self): \n",
    "        return iter(self.custom_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len( self.custom_indices )\n",
    "\n",
    "def parse_1_data(data):\n",
    "    \"\"\" \n",
    "    data: at time t from dataset.\n",
    "    #each timestamp can contain multiple uid because of obs_horizon\n",
    "    \"\"\"\n",
    "    if 'demo_no' not in data['obs']:\n",
    "        raise Exception(\"Please add demo_no and index_in_demo to the obs first.\")\n",
    "    \n",
    "    demo_nos = data['obs']['demo_no']\n",
    "    indices_in_demo = data['obs']['index_in_demo']\n",
    "    return demo_nos, indices_in_demo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "410b1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainRobomimicLowdimWorkspace(BaseWorkspace):\n",
    "    include_keys = ['global_step', 'epoch']\n",
    "\n",
    "    def __init__(self, cfg: OmegaConf):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # set seed\n",
    "        seed = cfg.training.seed\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # configure model\n",
    "        self.model: RobomimicLowdimPolicy = hydra.utils.instantiate(cfg.policy)\n",
    "\n",
    "        # configure training state\n",
    "        self.global_step = 0\n",
    "        self.epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db227aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self, save_rollout=False, remove_demos=[], segs_toremove={}):\n",
    "    cfg = copy.deepcopy(self.cfg)\n",
    "\n",
    "    # resume training\n",
    "    if cfg.training.resume:\n",
    "        lastest_ckpt_path = self.get_checkpoint_path()\n",
    "        if lastest_ckpt_path.is_file():\n",
    "            print(f\"Resuming from checkpoint {lastest_ckpt_path}\")\n",
    "            self.load_checkpoint(path=lastest_ckpt_path)\n",
    "\n",
    "    # configure dataset\n",
    "    dataset: BaseLowdimDataset\n",
    "    # dataset = hydra.utils.instantiate(cfg.task.dataset)\n",
    "\n",
    "    cfg_dataset={key: value for key, value in cfg.task.dataset.items() }\n",
    "    cfg_dataset['remove_demos'] = remove_demos\n",
    "    # dataset= hydra.utils.instantiate(cfg_dataset)\n",
    "\n",
    "\n",
    "    self.remove_ids={}                    # all the ids to remove for the key\n",
    "    self.segs_toremove = segs_toremove    #[start,end]\n",
    "\n",
    "    for key in segs_toremove.keys():\n",
    "        segs = segs_toremove[key]\n",
    "        ids = [] \n",
    "        for start, end in segs:\n",
    "            ids.extend(range(start, end + 1))  # Include the end value\n",
    "        self.remove_ids[key]=ids\n",
    "\n",
    "\n",
    "\n",
    "    # assert isinstance(dataset, BaseLowdimDataset)\n",
    "    # train_dataloader = DataLoader(dataset, **cfg.dataloader)\n",
    "\n",
    "\n",
    "    if len(self.remove_ids)==0:\n",
    "        print('---------******--------full traj dataset---------******--------')\n",
    "        dataset = hydra.utils.instantiate(cfg.task.dataset)\n",
    "        assert isinstance(dataset, BaseLowdimDataset)\n",
    "        train_dataloader = DataLoader(dataset, **cfg.dataloader) \n",
    "    else:\n",
    "        print('---------++++++++--------partial traj dataset---------++++++++--------') \n",
    "        new_config = {key: value for key, value in cfg.task.dataset.items()} \n",
    "        # new_config['obs_keys'] = {key: value for key, value in new_config['obs_keys'].items()}\n",
    "        # obs_shape_meta_config = {key: value for key, value in new_config['obs_keys'].items()}\n",
    "        # obs_shape_meta_config['demo_no'] = {'shape': [], 'type': 'low_dim'}\n",
    "        # obs_shape_meta_config['index_in_demo'] = {'shape': [], 'type': 'low_dim'}\n",
    "        # new_config['obs_keys'] = obs_shape_meta_config\n",
    "        # print('-----------------new_config obs_keys: ', new_config['obs_keys'])\n",
    "        new_config['obs_keys'].extend(['demo_no', 'index_in_demo'])  # add helper keys for demo number and index in demo\n",
    "\n",
    "\n",
    "        # dataset = hydra.utils.instantiate(cfg.task.dataset)\n",
    "        dataset = hydra.utils.instantiate(new_config)\n",
    "        assert isinstance(dataset, BaseLowdimDataset)\n",
    "\n",
    "\n",
    "        # valid_indices =[]  #in the dataset.\n",
    "        # print('generating valid indices ...')\n",
    "        # for index in tqdm( range(len(dataset)) ):\n",
    "        #     data = dataset.__getitem__(index)\n",
    "        #     demo_no, indices_in_demo = parse_1_data(data) \n",
    "\n",
    "        #     assert torch.all( demo_no[0]==demo_no[1] )                 #obs history from same demo\n",
    "        #     demo_name=f'demo_{int(demo_no[0])}'\n",
    "        #     ids = indices_in_demo.numpy().astype(int)\n",
    "            \n",
    "        #     should_remove = False\n",
    "        #     if demo_name in self.remove_ids:\n",
    "        #         should_remove = bool(set(self.remove_ids[demo_name]) & set(ids))\n",
    "        #     if should_remove: continue \n",
    "        #     valid_indices.append(index)\n",
    "\n",
    "        # print(f'Valid indices: {len(valid_indices)} / {len(dataset)} ')\n",
    "        # dataset.lowdim_keys.remove('demo_no')\n",
    "        # dataset.lowdim_keys.remove('index_in_demo')  # remove the added helper keys (demo_no and index in demo)\n",
    "\n",
    "        # sampler = CustomIndicesSampler(valid_indices)\n",
    "        # new_config = {key:value for key,value in cfg.dataloader.items()}\n",
    "        # new_config['shuffle'] = False\n",
    "        # new_config['sampler'] = sampler \n",
    "\n",
    "        # train_dataloader = DataLoader(dataset, **new_config) \n",
    "        train_dataloader = DataLoader(dataset, **cfg.dataloader) \n",
    "        # -----------------end of partial traj dataloader -----------------------\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    normalizer = dataset.get_normalizer()\n",
    "\n",
    "    # configure validation dataset\n",
    "    val_dataset = dataset.get_validation_dataset()\n",
    "    val_dataloader = DataLoader(val_dataset, **cfg.val_dataloader)\n",
    "\n",
    "    self.model.set_normalizer(normalizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a2d7562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['obs']\n",
      "using obs modality: rgb with keys: []\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n"
     ]
    }
   ],
   "source": [
    "ws= TrainRobomimicLowdimWorkspace(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "477c0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.run(save_rollout=False, remove_demos=[], segs_toremove=segs_toremove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "145b86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = copy.deepcopy(ws.cfg)\n",
    "remove_demos=[]\n",
    "\n",
    "# configure dataset\n",
    "dataset: BaseLowdimDataset\n",
    "# dataset = hydra.utils.instantiate(cfg.task.dataset)\n",
    "\n",
    "cfg_dataset={key: value for key, value in cfg.task.dataset.items() }\n",
    "cfg_dataset['remove_demos'] = remove_demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45718d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d82673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self=ws\n",
    "\n",
    "self.remove_ids={}                    # all the ids to remove for the key\n",
    "self.segs_toremove = segs_toremove    #[start,end]\n",
    "\n",
    "for key in segs_toremove.keys():\n",
    "    segs = segs_toremove[key]\n",
    "    ids = [] \n",
    "    for start, end in segs:\n",
    "        ids.extend(range(start, end + 1))  # Include the end value\n",
    "    self.remove_ids[key]=ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13a4a020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(self.remove_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28249954",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config = {key: value for key, value in cfg.task.dataset.items()} \n",
    "new_config['obs_keys'].extend(['demo_no', 'index_in_demo'])  # add helper keys for demo number and index in demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebffdbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------total 300 demos, hdf5_filter_key=None------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading hdf5 to ReplayBuffer: 100%|██████████| 300/300 [00:00<00:00, 900.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# dataset = hydra.utils.instantiate(new_config)\n",
    "del new_config['_target_']\n",
    "\n",
    "from diffusion_policy.dataset.robomimic_replay_lowdim_dataset import RobomimicReplayLowdimDataset\n",
    "dataset = RobomimicReplayLowdimDataset(**new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f313dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<diffusion_policy.dataset.robomimic_replay_lowdim_dataset.RobomimicReplayLowdimDataset at 0x7ff0163f0460>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6005d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49ddb661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------++++++++--------partial traj dataset---------++++++++--------\n"
     ]
    }
   ],
   "source": [
    "print('---------++++++++--------partial traj dataset---------++++++++--------') \n",
    "\n",
    "# new_config['obs_keys'] = {key: value for key, value in new_config['obs_keys'].items()}\n",
    "# obs_shape_meta_config = {key: value for key, value in new_config['obs_keys'].items()}\n",
    "# obs_shape_meta_config['demo_no'] = {'shape': [], 'type': 'low_dim'}\n",
    "# obs_shape_meta_config['index_in_demo'] = {'shape': [], 'type': 'low_dim'}\n",
    "# new_config['obs_keys'] = obs_shape_meta_config\n",
    "# print('-----------------new_config obs_keys: ', new_config['obs_keys'])\n",
    "\n",
    "\n",
    "# dataset = hydra.utils.instantiate(cfg.task.dataset)\n",
    "\n",
    "assert isinstance(dataset, BaseLowdimDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50afe81b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d53f088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531da523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6dbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f8a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
